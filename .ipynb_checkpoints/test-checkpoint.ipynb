{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9310b49-560b-4ff7-ac5f-a85b4a69fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6988649f-3190-4934-8bbd-e6d3ffe5ae24",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'intents.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2468/1675308999.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mintents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'intents.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intents.json'"
     ]
    }
   ],
   "source": [
    "## intializeing lemmatizer, loading json, creating lists, setting characters to ignore in our training set\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words, classes, documents = [], [] ,[]\n",
    "ignore_characters = ['?', '!', ',', '.']\n",
    "\n",
    "## organizing/clean/tokenizing json data\n",
    "\n",
    "for intent in intents['intents']:\n",
    "\tfor pattern in intent['patterns']:\n",
    "\t\tword_list = nltk.word_tokenize(pattern)\n",
    "\t\twords.extend(word_list)\n",
    "\t\tdocuments.append((word_list, intent['tag']))\n",
    "\t\tif intent['tag'] not in classes:\n",
    "\t\t\tclasses.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_characters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# print(words)\n",
    "\n",
    "## Saving data for words/classes into pkl\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "## converting words into numerical values, shuffling, and creating model with numerical data\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "\tbag = []\n",
    "\tword_patterns = document[0]\n",
    "\tword_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "\tfor word in words:\n",
    "\t\tbag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "\toutput_row = list(output_empty)\n",
    "\toutput_row[classes.index(document[1])] = 1\n",
    "\ttraining.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "## training neural network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs= 200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44a86c-1056-4b24-8825-3225e921f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c490c3-c2ac-424c-a3d9-2cd1eaa8c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading in data from model\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "##cleaning up sentences\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "\tsentence_words = nltk.word_tokenize(sentence)\n",
    "\tsentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "\treturn sentence_words\n",
    "\n",
    "##getting bag of words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "\tsentence_words = clean_up_sentence(sentence)\n",
    "\tbag = [0] * len(words)\n",
    "\tfor w in sentence_words:\n",
    "\t\tfor i, word in enumerate(words):\n",
    "\t\t\tif word == w:\n",
    "\t\t\t\tbag[i] = 1\n",
    "\treturn np.array(bag)\n",
    "\n",
    "##predicting class\n",
    "\n",
    "def predict_class(sentence):\n",
    "\tbow = bag_of_words(sentence)\n",
    "\tres = model.predict(np.array([bow]))[0]\n",
    "\terror_threshold = .03\n",
    "\tresults = [[i, r] for i, r in enumerate(res) if r > error_threshold]\n",
    "\n",
    "\tresults.sort(key=lambda x: x[1], reverse=True)\n",
    "\treturn_list = []\n",
    "\tfor r in results:\n",
    "\t\treturn_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
    "\treturn return_list\n",
    "\n",
    "##getting responce\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "\ttag = intents_list[0]['intent']\n",
    "\tlist_of_intents = intents_json['intents']\n",
    "\tfor i in list_of_intents:\n",
    "\t\tif i['tag'] == tag:\n",
    "\t\t\tfollow_ups_ = i['followups']\n",
    "\t\t\tresult = [(random.choice(i['responses0'])+random.choice(i['responses1'])+random.choice(i['responses2'])).format(name), follow_ups_]\n",
    "\t\t\tbreak\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961baf92-04bd-462b-abc8-0580980fd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_responses(var_name):\n",
    "\tlist0_, list1_ = [], intents['intents']\n",
    "\tfor i in list1_:\n",
    "\t\tif i['tag'] == var_name:\n",
    "\t\t\tfor s0_ in i['responses0']:\n",
    "\t\t\t\tfor s1_ in i['responses1']:\n",
    "\t\t\t\t\tfor s2_ in i['responses2']:\n",
    "\t\t\t\t\t\tlist0_.append(s0_ + s1_ + s2_)\n",
    "\treturn(list0_)\n",
    "get_all_responses('training questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563519d4-5290-4fe2-aad1-0df692917cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variable finder\n",
    "\n",
    "def var_finder(lst, var_searching, return_var, fail_var):\n",
    "    var_ = 0\n",
    "    for i in lst:\n",
    "        if i == var_searching:\n",
    "            var_ = 1\n",
    "            break\n",
    "    if var_ == 1:\n",
    "        return (return_var)\n",
    "    else:\n",
    "        return (fail_var)\n",
    "\n",
    "## getting all responses\n",
    "    \n",
    "def get_all_responses(var_name):\n",
    "\tlist0_, list1_ = [], intents['intents']\n",
    "\tfor i in list1_:\n",
    "\t\tif i['tag'] == var_name:\n",
    "\t\t\tfor s0_ in i['responses0']:\n",
    "\t\t\t\tfor s1_ in i['responses1']:\n",
    "\t\t\t\t\tfor s2_ in i['responses2']:\n",
    "\t\t\t\t\t\tlist0_.append(s0_ + s1_ + s2_)\n",
    "\treturn(list0_)\n",
    "        \n",
    "\n",
    "print('Bot is on, say hi!')\n",
    "\n",
    "## remove this when you are done bridging gate between app and server\n",
    "name = \"Marcos\"\n",
    "\n",
    "res = 'temp'\n",
    "\n",
    "while True:\n",
    "\tadd_to_database = get_all_responses('add to database now')\n",
    "\tmessage = spell(input('').lower())\n",
    "\n",
    "\t## test variable, remove before release\n",
    "\tif message == 'break':\n",
    "\t\tprint('turning off')\n",
    "\t\tbreak\n",
    "\n",
    "    ## bot training and storing after training\n",
    "\telif res == var_finder(add_to_database, res, res, False):\n",
    "\t\tmessage_0 = input('').lower()\n",
    "\t\tlist_temp = []\n",
    "\t\tlist_temp.append(message)\n",
    "\t\tlist_temp.append(message_0)\n",
    "\t\tres = get_response([{'intent': 'training done', 'probability': '1'}], intents)[0]\n",
    "\t\tfollowups = get_response([{'intent': 'training done', 'probability': '1'}], intents)[1]\n",
    "\t\tprint(res)\n",
    "\t\tif followups != \"\":\n",
    "\t\t\tprint(followups)\n",
    "\n",
    "\telse:\n",
    "\t\tints = predict_class(message)\n",
    "        ## bot is certain enough its the correct response \n",
    "\t\tprint(float(ints[0]['probability']))\n",
    "\t\tif float(ints[0]['probability']) > .9:\n",
    "\t\t\tres = get_response(ints, intents)[0]\n",
    "\t\t\tfollowups = get_response(ints, intents)[1]\n",
    "        ## bot is uncertain answer is correct result\n",
    "\t\telse:\n",
    "\t\t\tprint(message)\n",
    "\t\t\tres = get_response([{'intent': 'bot uncertain', 'probability': '1'}], intents)[0]\n",
    "\t\t\tfollowups = get_response([{'intent': 'bot uncertain', 'probability': '1'}], intents)[1]\n",
    "\t\tprint(res)\n",
    "\t\tif followups != \"\":\n",
    "\t\t\tprint(followups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dcfff-c6af-488c-9bf5-c398893a1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "\tbow = bag_of_words(sentence)\n",
    "\tres = model.predict(np.array([bow]))[0]\n",
    "\terror_threshold = .05\n",
    "\tresults = [[i, r] for i, r in enumerate(res) if r > error_threshold]\n",
    "\n",
    "\tresults.sort(key=lambda x: x[1], reverse=True)\n",
    "\treturn_list = []\n",
    "\tfor r in results:\n",
    "\t\treturn_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
    "\treturn return_list\n",
    "\n",
    "predict_class('')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
